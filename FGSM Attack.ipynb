{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae2e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e6a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d43a57fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56c41dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    saved_path=\"saved/random.pt\",\n",
    "    best_saved_path = \"saved/random_best.pt\",\n",
    "    lr=0.001, \n",
    "    EPOCHS = 3,\n",
    "    BATCH_SIZE = 32,\n",
    "    IMAGE_SIZE = 32,\n",
    "    TRAIN_VALID_SPLIT = 0.2,\n",
    "    device=device,\n",
    "    SEED = 42,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    USE_AMP = True,\n",
    "    channels_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57067a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(config['SEED'])\n",
    "# If you or any of the libraries you are using rely on NumPy, you can seed the global NumPy RNG \n",
    "np.random.seed(config['SEED'])\n",
    "# Prevent RNG for CPU and GPU using torch\n",
    "torch.manual_seed(config['SEED'])\n",
    "torch.cuda.manual_seed(config['SEED'])\n",
    "torch.backends.cudnn.benchmarks = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f53cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop((config['IMAGE_SIZE'],config['IMAGE_SIZE'])),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((config['IMAGE_SIZE'],config['IMAGE_SIZE'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((config['IMAGE_SIZE'],config['IMAGE_SIZE'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "792890e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = torchvision.datasets.CIFAR10(root='../Images', train=True,\n",
    "                                        download=True, transform=data_transforms['train'])\n",
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size=32,shuffle=True, num_workers = config['num_workers'],\n",
    "                                          pin_memory = config['pin_memory'])\n",
    "\n",
    "test_data = torchvision.datasets.CIFAR10(root='../Images', train=False,\n",
    "                                       download=True, transform=data_transforms['test'])\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=32,shuffle=True, num_workers = config['num_workers'],\n",
    "                                          pin_memory = config['pin_memory'])\n",
    "valid_dl = test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0c1eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dd5b8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 8, 8, 3, 1, 5, 3, 7, 3, 9, 4, 5, 4, 4, 0, 2, 1, 8, 6, 9, 5, 5, 0, 9,\n",
      "        9, 6, 6, 9, 5, 4, 7, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7768aac0f0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "a = iter(valid_dl)\n",
    "b = next(a)\n",
    "print(b[1])\n",
    "plt.imshow(b[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd19efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,criterion,optimizer,num_epochs=10):\n",
    "\n",
    "    since = time.time()                                            \n",
    "    batch_ct = 0\n",
    "    example_ct = 0\n",
    "    best_acc = 0.3\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        run_corrects = 0\n",
    "        #Training\n",
    "        model.train()\n",
    "        for x,y in train_dl: #BS=32 ([BS,3,224,224], [BS,4])            \n",
    "            if config['channels_last']:\n",
    "                x = x.to(config['device'], memory_format=torch.channels_last) #CHW --> #HWC\n",
    "            else:\n",
    "                x = x.to(config['device'])\n",
    "            y = y.to(config['device']) #CHW --> #HWC\n",
    "            \n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            #optimizer.zero_grad(set_to_none=True)\n",
    "            ######################################################################\n",
    "            \n",
    "            train_logits = model(x) #Input = [BS,3,224,224] (Image) -- Model --> [BS,4] (Output Scores)\n",
    "            \n",
    "            _, train_preds = torch.max(train_logits, 1)\n",
    "            train_loss = criterion(train_logits,y)\n",
    "            train_loss = criterion(train_logits,y)\n",
    "            run_corrects += torch.sum(train_preds == y.data)\n",
    "            \n",
    "            train_loss.backward() # Backpropagation this is where your W_gradient\n",
    "            loss=train_loss\n",
    "\n",
    "            optimizer.step() # W_new = W_old - LR * W_gradient \n",
    "            example_ct += len(x) \n",
    "            batch_ct += 1\n",
    "            if ((batch_ct + 1) % 400) == 0:\n",
    "                train_log(loss, example_ct, epoch)\n",
    "            ########################################################################\n",
    "        \n",
    "        #validation\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total = 0\n",
    "        # Disable gradient calculation for validation or inference using torch.no_rad()\n",
    "        with torch.no_grad():\n",
    "            for x,y in valid_dl:\n",
    "                if config['channels_last']:\n",
    "                    x = x.to(config['device'], memory_format=torch.channels_last) #CHW --> #HWC\n",
    "                else:\n",
    "                    x = x.to(config['device'])\n",
    "                y = y.to(config['device'])\n",
    "                valid_logits = model(x)\n",
    "                _, valid_preds = torch.max(valid_logits, 1)\n",
    "                valid_loss = criterion(valid_logits,y)\n",
    "                running_loss += valid_loss.item() * x.size(0)\n",
    "                running_corrects += torch.sum(valid_preds == y.data)\n",
    "                total += y.size(0)\n",
    "            \n",
    "        epoch_loss = running_loss / len(valid_data)\n",
    "        epoch_acc = running_corrects.double() / len(valid_data)\n",
    "        train_acc = run_corrects.double() / len(train_data)\n",
    "        print(\"Train Accuracy\",train_acc.cpu())\n",
    "        print(\"Validation Loss is {}\".format(epoch_loss))\n",
    "        print(\"Validation Accuracy is {}\\n\".format(epoch_acc.cpu()))\n",
    "        if epoch_acc.cpu()>best_acc:\n",
    "            print('One of the best validation accuracy found.\\n')\n",
    "            #torch.save(model.state_dict(), config['best_saved_path'])\n",
    "            best_acc = epoch_acc.cpu()\n",
    "\n",
    "            \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    #torch.save(model.state_dict(), config['saved_path'])\n",
    "\n",
    "    \n",
    "def train_log(loss, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bba03122",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(64 * 2 * 2, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 2 * 2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model and set it to the GPU if available\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15e1908a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "Loss after 12768 examples: 1.871\n",
      "Loss after 25568 examples: 1.927\n",
      "Loss after 38368 examples: 1.923\n",
      "Train Accuracy tensor(0.3144, dtype=torch.float64)\n",
      "Validation Loss is 1.4533152322769165\n",
      "Validation Accuracy is 0.4682\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "Loss after 51152 examples: 1.729\n",
      "Loss after 63952 examples: 1.740\n",
      "Loss after 76752 examples: 1.231\n",
      "Loss after 89552 examples: 1.395\n",
      "Train Accuracy tensor(0.4025, dtype=torch.float64)\n",
      "Validation Loss is 1.3180529211044312\n",
      "Validation Accuracy is 0.5306000000000001\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "Loss after 102336 examples: 1.710\n",
      "Loss after 115136 examples: 1.803\n",
      "Loss after 127936 examples: 1.578\n",
      "Loss after 140736 examples: 1.728\n",
      "Train Accuracy tensor(0.4510, dtype=torch.float64)\n",
      "Validation Loss is 1.2299341583251953\n",
      "Validation Accuracy is 0.5694\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "Loss after 153520 examples: 1.093\n",
      "Loss after 166320 examples: 1.701\n",
      "Loss after 179120 examples: 1.529\n",
      "Loss after 191920 examples: 1.337\n",
      "Train Accuracy tensor(0.4737, dtype=torch.float64)\n",
      "Validation Loss is 1.1778363109588623\n",
      "Validation Accuracy is 0.5905\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "Loss after 204704 examples: 1.458\n",
      "Loss after 217504 examples: 1.673\n",
      "Loss after 230304 examples: 1.146\n",
      "Loss after 243104 examples: 1.155\n",
      "Train Accuracy tensor(0.4864, dtype=torch.float64)\n",
      "Validation Loss is 1.2183500003814698\n",
      "Validation Accuracy is 0.5813\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "Loss after 255888 examples: 1.305\n",
      "Loss after 268688 examples: 1.399\n",
      "Loss after 281488 examples: 1.336\n",
      "Loss after 294288 examples: 1.492\n",
      "Train Accuracy tensor(0.5020, dtype=torch.float64)\n",
      "Validation Loss is 1.1806742356300355\n",
      "Validation Accuracy is 0.5931000000000001\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "Loss after 307072 examples: 2.215\n",
      "Loss after 319872 examples: 1.216\n",
      "Loss after 332672 examples: 1.703\n",
      "Loss after 345472 examples: 1.632\n",
      "Train Accuracy tensor(0.5096, dtype=torch.float64)\n",
      "Validation Loss is 1.0627446392059325\n",
      "Validation Accuracy is 0.6238\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "Loss after 358256 examples: 1.383\n",
      "Loss after 371056 examples: 1.414\n",
      "Loss after 383856 examples: 1.454\n",
      "Loss after 396656 examples: 1.557\n",
      "Train Accuracy tensor(0.5178, dtype=torch.float64)\n",
      "Validation Loss is 1.0891856101989745\n",
      "Validation Accuracy is 0.6242\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "Loss after 409440 examples: 1.207\n",
      "Loss after 422240 examples: 1.404\n",
      "Loss after 435040 examples: 0.977\n",
      "Loss after 447840 examples: 1.124\n",
      "Train Accuracy tensor(0.5242, dtype=torch.float64)\n",
      "Validation Loss is 1.1070686299324035\n",
      "Validation Accuracy is 0.6223000000000001\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "Loss after 460624 examples: 1.057\n",
      "Loss after 473424 examples: 1.667\n",
      "Loss after 486224 examples: 1.453\n",
      "Loss after 499024 examples: 1.547\n",
      "Train Accuracy tensor(0.5325, dtype=torch.float64)\n",
      "Validation Loss is 1.0626747463226318\n",
      "Validation Accuracy is 0.6283000000000001\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Training complete in 2m 14s\n"
     ]
    }
   ],
   "source": [
    "model = model.to(config['device'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=config['lr'])\n",
    "train_model(model,criterion,optimizer,num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c1c18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afc847b6",
   "metadata": {},
   "source": [
    "# FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb32dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b149942b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Epsilon =  0.03\n",
      "Accuracy on adversarial data: 24 %\n"
     ]
    }
   ],
   "source": [
    "def evaluate_fgsm(epsilon = 0.03):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    #with torch.no_grad():\n",
    "    for data in test_dl:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Set requires_grad flag to True\n",
    "        images.requires_grad = True\n",
    "\n",
    "        # Compute the gradient\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        model.zero_grad()\n",
    "        grad = torch.autograd.grad(loss, images, retain_graph=True, only_inputs=True)[0]\n",
    "        data_grad = grad.data\n",
    "\n",
    "        # Generate adversarial examples\n",
    "        perturbed_images = fgsm_attack(images, epsilon, data_grad)\n",
    "        outputs = model(perturbed_images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Reset requires_grad flag back to False\n",
    "        images.requires_grad = False\n",
    "    print('For Epsilon = ', epsilon)\n",
    "    print('Accuracy on adversarial data: %d %%' % (100 * correct / total))\n",
    "evaluate_fgsm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b3b8cbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Epsilon =  0\n",
      "Accuracy on adversarial data: 29 %\n",
      "\n",
      "For Epsilon =  0.01\n",
      "Accuracy on adversarial data: 27 %\n",
      "\n",
      "For Epsilon =  0.05\n",
      "Accuracy on adversarial data: 22 %\n",
      "\n",
      "For Epsilon =  0.1\n",
      "Accuracy on adversarial data: 16 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e in [0,0.01,0.05,0.1]:\n",
    "    evaluate_fgsm(e)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3718adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
