{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbb89e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a94298f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d262d4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 16 13:28:31 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.84       Driver Version: 460.84       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 20%   23C    P8     9W / 250W |    447MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:06:00.0 Off |                  N/A |\n",
      "| 20%   23C    P8     6W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:07:00.0 Off |                  N/A |\n",
      "| 20%   44C    P2    56W / 250W |   2921MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  Off  | 00000000:08:00.0 Off |                  N/A |\n",
      "| 20%   24C    P8     7W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce GTX 108...  Off  | 00000000:0C:00.0 Off |                  N/A |\n",
      "| 20%   33C    P8     7W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce GTX 108...  Off  | 00000000:0D:00.0 Off |                  N/A |\n",
      "| 20%   23C    P8     7W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce GTX 108...  Off  | 00000000:0E:00.0 Off |                  N/A |\n",
      "| 20%   50C    P0    57W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce GTX 108...  Off  | 00000000:0F:00.0 Off |                  N/A |\n",
      "| 56%   84C    P2   230W / 250W |   4149MiB / 11178MiB |     98%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     38504      C   /opt/conda/bin/python             445MiB |\n",
      "|    2   N/A  N/A     14515      C   python3                          2919MiB |\n",
      "|    7   N/A  N/A     34865      C   python3                          4147MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "760c4a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77711eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    saved_path=\"saved/random.pt\",\n",
    "    best_saved_path = \"saved/random_best.pt\",\n",
    "    lr=0.001, \n",
    "    EPOCHS = 3,\n",
    "    BATCH_SIZE = 32,\n",
    "    IMAGE_SIZE = 32,\n",
    "    TRAIN_VALID_SPLIT = 0.2,\n",
    "    device=device,\n",
    "    SEED = 42,\n",
    "    pin_memory=True,\n",
    "    num_workers=3,\n",
    "    USE_AMP = True,\n",
    "    channels_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8776495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(config['SEED'])\n",
    "# If you or any of the libraries you are using rely on NumPy, you can seed the global NumPy RNG \n",
    "np.random.seed(config['SEED'])\n",
    "# Prevent RNG for CPU and GPU using torch\n",
    "torch.manual_seed(config['SEED'])\n",
    "torch.cuda.manual_seed(config['SEED'])\n",
    "torch.backends.cudnn.benchmarks = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e0a06e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop((config['IMAGE_SIZE'],config['IMAGE_SIZE'])),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((config['IMAGE_SIZE'],config['IMAGE_SIZE'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((config['IMAGE_SIZE'],config['IMAGE_SIZE'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4462696f",
   "metadata": {},
   "source": [
    "# SHVN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82f304d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../Images/train_32x32.mat\n",
      "Using downloaded and verified file: ../Images/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "train_data = torchvision.datasets.SVHN(root='../Images', split = 'train',\n",
    "                                        download=True, transform=data_transforms['train'])\n",
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size=32,shuffle=True, num_workers = config['num_workers'],\n",
    "                                          pin_memory = config['pin_memory'])\n",
    "\n",
    "test_data = torchvision.datasets.SVHN(root='../Images', split='test',\n",
    "                                       download=True, transform=data_transforms['test'])\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=32,shuffle=True, num_workers = config['num_workers'],\n",
    "                                          pin_memory = config['pin_memory'])\n",
    "valid_dl = test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f7d392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = test_data\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13c5c8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 7, 0, 9, 1, 2, 9, 3, 1, 9, 9, 1, 1, 7, 8, 7, 3, 8, 6, 4, 2, 2, 5, 3,\n",
      "        5, 2, 8, 0, 4, 2, 1, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb08b560748>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "a = iter(valid_dl)\n",
    "b = next(a)\n",
    "print(b[1])\n",
    "plt.imshow(b[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2d105a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,criterion,optimizer,num_epochs=10):\n",
    "\n",
    "    since = time.time()                                            \n",
    "    batch_ct = 0\n",
    "    example_ct = 0\n",
    "    best_acc = 0.3\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        run_corrects = 0\n",
    "        #Training\n",
    "        model.train()\n",
    "        for x,y in train_dl: #BS=32 ([BS,3,224,224], [BS,4])            \n",
    "            if config['channels_last']:\n",
    "                x = x.to(config['device'], memory_format=torch.channels_last) #CHW --> #HWC\n",
    "            else:\n",
    "                x = x.to(config['device'])\n",
    "            y = y.to(config['device']) #CHW --> #HWC\n",
    "            \n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            #optimizer.zero_grad(set_to_none=True)\n",
    "            ######################################################################\n",
    "            \n",
    "            train_logits = model(x) #Input = [BS,3,224,224] (Image) -- Model --> [BS,4] (Output Scores)\n",
    "            \n",
    "            _, train_preds = torch.max(train_logits, 1)\n",
    "            train_loss = criterion(train_logits,y)\n",
    "            train_loss = criterion(train_logits,y)\n",
    "            run_corrects += torch.sum(train_preds == y.data)\n",
    "            \n",
    "            train_loss.backward() # Backpropagation this is where your W_gradient\n",
    "            loss=train_loss\n",
    "\n",
    "            optimizer.step() # W_new = W_old - LR * W_gradient \n",
    "            example_ct += len(x) \n",
    "            batch_ct += 1\n",
    "            if ((batch_ct + 1) % 400) == 0:\n",
    "                train_log(loss, example_ct, epoch)\n",
    "            ########################################################################\n",
    "        \n",
    "        #validation\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total = 0\n",
    "        # Disable gradient calculation for validation or inference using torch.no_rad()\n",
    "        with torch.no_grad():\n",
    "            for x,y in valid_dl:\n",
    "                if config['channels_last']:\n",
    "                    x = x.to(config['device'], memory_format=torch.channels_last) #CHW --> #HWC\n",
    "                else:\n",
    "                    x = x.to(config['device'])\n",
    "                y = y.to(config['device'])\n",
    "                valid_logits = model(x)\n",
    "                _, valid_preds = torch.max(valid_logits, 1)\n",
    "                valid_loss = criterion(valid_logits,y)\n",
    "                running_loss += valid_loss.item() * x.size(0)\n",
    "                running_corrects += torch.sum(valid_preds == y.data)\n",
    "                total += y.size(0)\n",
    "            \n",
    "        epoch_loss = running_loss / len(valid_data)\n",
    "        epoch_acc = running_corrects.double() / len(valid_data)\n",
    "        train_acc = run_corrects.double() / len(train_data)\n",
    "        print(\"Train Accuracy\",train_acc.cpu())\n",
    "        print(\"Validation Loss is {}\".format(epoch_loss))\n",
    "        print(\"Validation Accuracy is {}\\n\".format(epoch_acc.cpu()))\n",
    "        if epoch_acc.cpu()>best_acc:\n",
    "            print('One of the best validation accuracy found.\\n')\n",
    "            #torch.save(model.state_dict(), config['best_saved_path'])\n",
    "            best_acc = epoch_acc.cpu()\n",
    "\n",
    "            \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    #torch.save(model.state_dict(), config['saved_path'])\n",
    "\n",
    "    \n",
    "def train_log(loss, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad64384",
   "metadata": {},
   "source": [
    "# Architecture-1: Squeezenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a02e49ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezenet = torchvision.models.squeezenet1_0(pretrained=True)\n",
    "squeezenet.classifier[1] = nn.Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
    "model = squeezenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cd17557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "Loss after 12768 examples: 1.876\n",
      "Loss after 25568 examples: 1.662\n",
      "Loss after 38368 examples: 2.146\n",
      "Loss after 51168 examples: 1.964\n",
      "Loss after 63968 examples: 1.876\n",
      "Train Accuracy tensor(0.3441, dtype=torch.float64)\n",
      "Validation Loss is 1.340528725112371\n",
      "Validation Accuracy is 0.5864320835894283\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "Loss after 76745 examples: 1.507\n",
      "Loss after 89545 examples: 1.902\n",
      "Loss after 102345 examples: 1.244\n",
      "Loss after 115145 examples: 1.330\n",
      "Loss after 127945 examples: 1.329\n",
      "Loss after 140745 examples: 0.949\n",
      "Train Accuracy tensor(0.5045, dtype=torch.float64)\n",
      "Validation Loss is 1.0833171721463124\n",
      "Validation Accuracy is 0.6660264290104486\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "Loss after 153522 examples: 1.695\n",
      "Loss after 166322 examples: 1.447\n",
      "Loss after 179122 examples: 1.751\n",
      "Loss after 191922 examples: 1.040\n",
      "Loss after 204722 examples: 1.898\n",
      "Loss after 217522 examples: 1.479\n",
      "Train Accuracy tensor(0.5353, dtype=torch.float64)\n",
      "Validation Loss is 1.0456146497483185\n",
      "Validation Accuracy is 0.6901505838967424\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "Loss after 230299 examples: 1.624\n",
      "Loss after 243099 examples: 1.325\n",
      "Loss after 255899 examples: 1.734\n",
      "Loss after 268699 examples: 0.992\n",
      "Loss after 281499 examples: 0.959\n",
      "Train Accuracy tensor(0.5427, dtype=torch.float64)\n",
      "Validation Loss is 1.0479919092011876\n",
      "Validation Accuracy is 0.6826213890596189\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "Loss after 294276 examples: 1.504\n",
      "Loss after 307076 examples: 1.688\n",
      "Loss after 319876 examples: 1.760\n",
      "Loss after 332676 examples: 1.327\n",
      "Loss after 345476 examples: 1.185\n",
      "Loss after 358276 examples: 1.563\n",
      "Train Accuracy tensor(0.5499, dtype=torch.float64)\n",
      "Validation Loss is 1.0111514548619394\n",
      "Validation Accuracy is 0.6826598033189919\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "Loss after 371053 examples: 1.533\n",
      "Loss after 383853 examples: 1.549\n",
      "Loss after 396653 examples: 1.315\n",
      "Loss after 409453 examples: 1.545\n",
      "Loss after 422253 examples: 1.910\n",
      "Loss after 435053 examples: 1.540\n",
      "Train Accuracy tensor(0.5540, dtype=torch.float64)\n",
      "Validation Loss is 1.0358526748459107\n",
      "Validation Accuracy is 0.6790872771972956\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "Loss after 447830 examples: 1.284\n",
      "Loss after 460630 examples: 1.438\n",
      "Loss after 473430 examples: 1.531\n",
      "Loss after 486230 examples: 1.655\n",
      "Loss after 499030 examples: 1.391\n",
      "Loss after 511830 examples: 1.608\n",
      "Train Accuracy tensor(0.5548, dtype=torch.float64)\n",
      "Validation Loss is 0.9970500105425266\n",
      "Validation Accuracy is 0.6916103257529195\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "Loss after 524607 examples: 1.781\n",
      "Loss after 537407 examples: 1.321\n",
      "Loss after 550207 examples: 1.376\n",
      "Loss after 563007 examples: 1.287\n",
      "Loss after 575807 examples: 1.327\n",
      "Train Accuracy tensor(0.5595, dtype=torch.float64)\n",
      "Validation Loss is 1.0161696243447325\n",
      "Validation Accuracy is 0.6906115550092193\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "Loss after 588584 examples: 1.336\n",
      "Loss after 601384 examples: 1.295\n",
      "Loss after 614184 examples: 1.776\n",
      "Loss after 626984 examples: 1.055\n",
      "Loss after 639784 examples: 1.758\n",
      "Loss after 652584 examples: 1.427\n",
      "Train Accuracy tensor(0.5601, dtype=torch.float64)\n",
      "Validation Loss is 1.0036125110568777\n",
      "Validation Accuracy is 0.686040258143823\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "Loss after 665361 examples: 1.436\n",
      "Loss after 678161 examples: 1.113\n",
      "Loss after 690961 examples: 0.976\n",
      "Loss after 703761 examples: 1.037\n",
      "Loss after 716561 examples: 1.745\n",
      "Loss after 729361 examples: 1.409\n",
      "Train Accuracy tensor(0.5633, dtype=torch.float64)\n",
      "Validation Loss is 1.0300154278504754\n",
      "Validation Accuracy is 0.6910725261216963\n",
      "\n",
      "Training complete in 8m 15s\n"
     ]
    }
   ],
   "source": [
    "model = model.to(config['device'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=config['lr'])\n",
    "train_model(model,criterion,optimizer,num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da57b159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cd3199a",
   "metadata": {},
   "source": [
    "# Mask Based attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab7609ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_based_attack(model, images, labels, epsilon=0.03):\n",
    "    model.eval()\n",
    "    images = images.clone().detach().requires_grad_(True)\n",
    "    outputs = model(images)\n",
    "    loss = F.cross_entropy(outputs, labels)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    grad = images.grad.data\n",
    "    sign = grad.sign()\n",
    "    perturbed_images = images + epsilon * sign\n",
    "    perturbed_images = torch.clamp(perturbed_images, 0, 1)\n",
    "    return perturbed_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c32f313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Epsilon =  0.03\n",
      "Accuracy on perturbed mask attack data: 48 %\n"
     ]
    }
   ],
   "source": [
    "def evaluate_mask_attack(epsilon = 0.03):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    #with torch.no_grad():\n",
    "    for data in test_dl:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Set requires_grad flag to True\n",
    "        images.requires_grad = True\n",
    "\n",
    "        # Compute the gradient\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        model.zero_grad()\n",
    "        grad = torch.autograd.grad(loss, images, retain_graph=True, only_inputs=True)[0]\n",
    "        data_grad = grad.data\n",
    "\n",
    "        # Generate adversarial examples\n",
    "        perturbed_images = mask_based_attack(model, images, labels, epsilon)\n",
    "        outputs = model(perturbed_images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Reset requires_grad flag back to False\n",
    "        images.requires_grad = False\n",
    "    print('For Epsilon = ', epsilon)\n",
    "    print('Accuracy on perturbed mask attack data: %d %%' % (100 * correct / total))\n",
    "evaluate_mask_attack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2be57eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Epsilon =  0\n",
      "Accuracy on perturbed mask attack data: 54 %\n",
      "\n",
      "For Epsilon =  0.01\n",
      "Accuracy on perturbed mask attack data: 52 %\n",
      "\n",
      "For Epsilon =  0.05\n",
      "Accuracy on perturbed mask attack data: 43 %\n",
      "\n",
      "For Epsilon =  0.1\n",
      "Accuracy on perturbed mask attack data: 35 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e in [0,0.01,0.05,0.1]:\n",
    "    evaluate_mask_attack(e)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c91cab",
   "metadata": {},
   "source": [
    "# PGD Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc243b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(images, targets, net, criterion, epsilon=0.1, alpha=0.01, num_iter=40):\n",
    "#     images = images.clone().detach().to(device)\n",
    "#     targets = targets.clone().detach().to(device)\n",
    "\n",
    "    # Create a perturbation vector of the same shape as the input images\n",
    "    perturbation = torch.zeros_like(images).to(device)\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        # Zero out the gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # Compute the loss and the gradients of the loss with respect to the inputs\n",
    "        loss = criterion(net(images + perturbation), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # Add the gradient of the loss to the perturbation vector\n",
    "        perturbation = (perturbation + alpha * torch.sign(images.grad.data)).clamp(-epsilon, epsilon)\n",
    "        \n",
    "        # Clamp the perturbed images to the valid range\n",
    "        perturbed_images = (images + perturbation).clamp(0, 1)\n",
    "\n",
    "        # Zero out the gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # Compute the loss and the gradients of the loss with respect to the inputs\n",
    "        loss = criterion(net(perturbed_images), targets)\n",
    "        loss.backward()\n",
    "\n",
    "    # Return the perturbed images\n",
    "    return perturbed_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "536dd1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Epsilon =  0.03\n",
      "Accuracy on PGD attacked data: 35 %\n"
     ]
    }
   ],
   "source": [
    "def evaluate_pgd(epsilon = 0.03):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    #with torch.no_grad():\n",
    "    for data in test_dl:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Set requires_grad flag to True\n",
    "        images.requires_grad = True\n",
    "\n",
    "        # Compute the gradient\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Generate adversarial examples\n",
    "        perturbed_images = pgd_attack(images, labels, model, criterion, epsilon)\n",
    "        outputs = model(perturbed_images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Reset requires_grad flag back to False\n",
    "        images.requires_grad = False\n",
    "\n",
    "    print('For Epsilon = ', epsilon)\n",
    "    print('Accuracy on PGD attacked data: %d %%' % (100 * correct / total))\n",
    "evaluate_pgd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c1810a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Epsilon =  0\n",
      "Accuracy on PGD attacked data: 54 %\n",
      "\n",
      "For Epsilon =  0.01\n",
      "Accuracy on PGD attacked data: 47 %\n",
      "\n",
      "For Epsilon =  0.05\n",
      "Accuracy on PGD attacked data: 26 %\n",
      "\n",
      "For Epsilon =  0.1\n",
      "Accuracy on PGD attacked data: 15 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e in [0,0.01,0.05,0.1]:\n",
    "    evaluate_pgd(e)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9af650",
   "metadata": {},
   "source": [
    "# Architecture-2: Shufflenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "badb6445",
   "metadata": {},
   "outputs": [],
   "source": [
    "shufflenet = models.shufflenet_v2_x1_0(pretrained = True)\n",
    "shufflenet.fc = nn.Linear(in_features = 1024, out_features = 10, bias=True)\n",
    "model = shufflenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4486e533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "Loss after 12768 examples: 1.432\n",
      "Loss after 25568 examples: 1.355\n",
      "Loss after 38368 examples: 1.067\n",
      "Loss after 51168 examples: 1.303\n",
      "Loss after 63968 examples: 0.873\n",
      "Train Accuracy tensor(0.5682, dtype=torch.float64)\n",
      "Validation Loss is 0.7145773142338092\n",
      "Validation Accuracy is 0.7768131530424093\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "Loss after 76745 examples: 0.871\n",
      "Loss after 89545 examples: 1.409\n",
      "Loss after 102345 examples: 0.890\n",
      "Loss after 115145 examples: 0.718\n",
      "Loss after 127945 examples: 1.178\n",
      "Loss after 140745 examples: 0.798\n",
      "Train Accuracy tensor(0.6633, dtype=torch.float64)\n",
      "Validation Loss is 0.579504482461839\n",
      "Validation Accuracy is 0.8250230485556238\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "Loss after 153522 examples: 1.327\n",
      "Loss after 166322 examples: 0.934\n",
      "Loss after 179122 examples: 1.094\n",
      "Loss after 191922 examples: 0.977\n",
      "Loss after 204722 examples: 0.651\n",
      "Loss after 217522 examples: 1.397\n",
      "Train Accuracy tensor(0.6886, dtype=torch.float64)\n",
      "Validation Loss is 0.5349847872197152\n",
      "Validation Accuracy is 0.8404271665642286\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "Loss after 230299 examples: 0.834\n",
      "Loss after 243099 examples: 1.038\n",
      "Loss after 255899 examples: 0.802\n",
      "Loss after 268699 examples: 1.043\n",
      "Loss after 281499 examples: 1.018\n",
      "Train Accuracy tensor(0.7054, dtype=torch.float64)\n",
      "Validation Loss is 0.5155076883234125\n",
      "Validation Accuracy is 0.8417332513829132\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "Loss after 294276 examples: 0.729\n",
      "Loss after 307076 examples: 1.005\n",
      "Loss after 319876 examples: 0.766\n",
      "Loss after 332676 examples: 1.165\n",
      "Loss after 345476 examples: 0.784\n",
      "Loss after 358276 examples: 0.647\n",
      "Train Accuracy tensor(0.7166, dtype=torch.float64)\n",
      "Validation Loss is 0.46112335793394654\n",
      "Validation Accuracy is 0.8599800245851259\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "Loss after 371053 examples: 0.766\n",
      "Loss after 383853 examples: 0.574\n",
      "Loss after 396653 examples: 0.685\n",
      "Loss after 409453 examples: 0.878\n",
      "Loss after 422253 examples: 0.937\n",
      "Loss after 435053 examples: 1.145\n",
      "Train Accuracy tensor(0.7227, dtype=torch.float64)\n",
      "Validation Loss is 0.4475821832992874\n",
      "Validation Accuracy is 0.8659342347879532\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "Loss after 447830 examples: 0.838\n",
      "Loss after 460630 examples: 0.591\n",
      "Loss after 473430 examples: 0.517\n",
      "Loss after 486230 examples: 0.655\n",
      "Loss after 499030 examples: 0.784\n",
      "Loss after 511830 examples: 0.874\n",
      "Train Accuracy tensor(0.7274, dtype=torch.float64)\n",
      "Validation Loss is 0.44255388728225603\n",
      "Validation Accuracy is 0.86339889366933\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "Loss after 524607 examples: 0.644\n",
      "Loss after 537407 examples: 1.604\n",
      "Loss after 550207 examples: 0.362\n",
      "Loss after 563007 examples: 0.810\n",
      "Loss after 575807 examples: 1.045\n",
      "Train Accuracy tensor(0.7337, dtype=torch.float64)\n",
      "Validation Loss is 0.4222806304162019\n",
      "Validation Accuracy is 0.8721573448063921\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "Loss after 588584 examples: 1.088\n",
      "Loss after 601384 examples: 0.818\n",
      "Loss after 614184 examples: 0.665\n",
      "Loss after 626984 examples: 0.959\n",
      "Loss after 639784 examples: 1.058\n",
      "Loss after 652584 examples: 0.966\n",
      "Train Accuracy tensor(0.7397, dtype=torch.float64)\n",
      "Validation Loss is 0.3891030370859405\n",
      "Validation Accuracy is 0.8801090964966195\n",
      "\n",
      "One of the best validation accuracy found.\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "Loss after 665361 examples: 1.100\n",
      "Loss after 678161 examples: 0.522\n",
      "Loss after 690961 examples: 0.826\n",
      "Loss after 703761 examples: 0.941\n",
      "Loss after 716561 examples: 0.561\n",
      "Loss after 729361 examples: 0.884\n",
      "Train Accuracy tensor(0.7434, dtype=torch.float64)\n",
      "Validation Loss is 0.40651249159276304\n",
      "Validation Accuracy is 0.8773432698217578\n",
      "\n",
      "Training complete in 20m 10s\n"
     ]
    }
   ],
   "source": [
    "model = model.to(config['device'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=config['lr'])\n",
    "train_model(model,criterion,optimizer,num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ac89515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Epsilon =  0\n",
      "Accuracy on perturbed mask attack data: 63 %\n",
      "\n",
      "For Epsilon =  0.01\n",
      "Accuracy on perturbed mask attack data: 55 %\n",
      "\n",
      "For Epsilon =  0.03\n",
      "Accuracy on perturbed mask attack data: 44 %\n",
      "\n",
      "For Epsilon =  0.05\n",
      "Accuracy on perturbed mask attack data: 37 %\n",
      "\n",
      "For Epsilon =  0.1\n",
      "Accuracy on perturbed mask attack data: 28 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mask Attack\n",
    "for e in [0,0.01,0.03,0.05,0.1]:\n",
    "    evaluate_mask_attack(e)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e249e634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Epsilon =  0\n",
      "Accuracy on PGD attacked data: 63 %\n",
      "\n",
      "For Epsilon =  0.01\n",
      "Accuracy on PGD attacked data: 40 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PGD Attack\n",
    "for e in [0,0.01,0.03,0.05,0.1]:\n",
    "    evaluate_pgd(e)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4df6b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
